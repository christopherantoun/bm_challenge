{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Notebook Description\n\nThe aim of this notebook is to create a fact table into a PostgreSQL-Database. \nThe fact table - *transaction* - will be used by data analysts to create reports and predictions about the companies performance. \nThe granularity of the table is transaction id - each row represents a unique transaction. \nThe table includes keys to the following dimensions:\n\n1. Customer (**customer_id**)\n2. Transaction Date (**transaction_date**) \n3. Payment Provider (**provider**)\n\nThe measure of the fact table is:\n\n1. Transaction Amount (**amount_eur**) \n\nThis would allow the data-analysts to create reports and predictions about the companies performance over selected time periods and per customer. The third dimension - payment provider - is needed since the transaction data is coming from two different payment providers. \n\nThe notebook should be triggered on a daily basis. This scheduling is dependent on the freshness interval of the payment data. Since payment data from Provider B is ingested on a daily basis, and this is the \"slowest\" of the two, the scheduling frequency of this notebook is set to be the same.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Notebook Skeleton: \n\n\n## Extract\n1. Extract  <br>\n\n    1.1. Load data from Payment Provider A using REST-API <br>\n    \n    1.2. Load data from Payment Provider B using CSVs \n\n## Transform\n\n2. Transform  <br>\n\n    2.1. **Compatibility**  <br>\n        2.1.1. Match transaction date format  <br>\n        2.1.2. Match the currency in \"amounts\"  <br>\n        2.1.3. Enforce compatibility with data ingestion frequency (daily vs. streaming)  <br>\n        \n    2.2. **Decryption** - Decrypt customer_id of both datasets <br>\n    \n    2.3. **Create bigger picture** <br>\n        2.3.1. Select common columns between the two datasets - limiting that to what is needed <br>\n        2.3.2. Create a new id that is unique across both datasets <br>\n        2.3.3. Concatenate the two datasets <br>\n\n## Load\n3. Load  <br>\n\n    3.1. Establish connection to Postgres database <br>\n    \n    3.2. Write fact table to database  <br>\n        3.2.1. Write from scratch if the table doesn't already exist <br>\n        3.2.2. Append to exisitng table <br>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Extract",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Extract payment data through REST-API",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import requests\nimport pandas as pd\nimport json\n\n# Specify API URL\napi_url = \"https://jsonplaceholder.typicode.com/todos/1\"\n\n# Send a GET request \nresponse = requests.get(api_url)\n\n# Get response in JSON\njson_data = response.json()\n\n# Convert JSON to Pandas DataFrame\npayment_provider_a_raw = pd.read_json(json_data, orient='index').T",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Extract payment data through CSV\n- The CSV files are stored locally \n- Since the CSV files are daily exports, the following directory structure is imposed\n- ```/provider_b/year/month/day/payment_data.csv```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nimport glob\nimport pandas as pd\nimport datetime\n\n# Set directory containing CSV files\nnow = datetime.datetime.now()\none_day = datetime.timedelta(days=1)\nload_time = now - one_day\nyear = load_time.year\nmonth = load_time.month\nday = load_time.day\n\ndirectory = f'/provider_b/{year}/{month}/{day}/payment_data'\n\n# Get all CSV files in directory\ncsv_files = glob.glob(os.path.join(directory, '*.csv'))\n\n# Loop through CSV files and read them into Pandas DataFrames\ndfs = []\nfor file in csv_files:\n    df = pd.read_csv(file)\n    dfs.append(df)\n\n# Concatenate all DataFrames into one\npayment_provider_b_raw = pd.concat(dfs)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Transform",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Ensure that the two dataframes comform when it comes to transaction timestamp.\ndef select_newest_timestamp(df):\n    df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n    return df.loc[df['transaction_date'] == df['transaction_date'].max(), 'timestamp'].iloc[0]\n\nnewest_timestamp = select_newest_timestamp(payment_provider_b)\n\n# Use this timestamp to filter out transactions from payment provider A that are newer\ndef filter_dataframe(df, timestamp):\n    df['transaction_timestamp'] = pd.to_datetime(df['transaction_timestamp'])\n    filtered_df = df[df['transaction_timestamp'] <= timestamp]\n    return filtered_df\n\npayment_provider_a_transformed = filter_dataframe(payment_provider_a_raw, newest_timestamp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create a column in both datasets that include the name of the payment provider\npayment_provider_a_transformed = payment_provider_a_transformed.assign(provider='A')\npayment_provider_b_transformed = payment_provider_b_transformed.assign(provider='B')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Since transaction_id is unique in each respective table, we can use this value hashed with 'provider' value\n# to create a new transaction_id that would be unique after the two dfs are concatenated\nimport hashlib\n\ndef generate_unique_id(row):\n    # Concatenate values of two columns\n    string_to_hash = str(row['id']) + str(row['provider'])\n    \n    # Apply hash function to generate unique ID\n    unique_id = hashlib.sha256(string_to_hash.encode()).hexdigest()\n    \n    return unique_id\n\n# Apply the generate_unique_id() function to each row of the dataframe\npayment_provider_a_transformed['id_hashed'] = payment_provider_a_transformed.apply(generate_unique_id, axis=1)\npayment_provider_b_transformed['id_hashed'] = payment_provider_a_transformed.apply(generate_unique_id, axis=1)\n\n# By the same logic, customer_id (present in both dfs) also needs to be regenerated to ensure uniqueness across the two tables\ndef generate_unique_customer_id(row):\n    # Concatenate values of two columns\n    string_to_hash = str(row['id']) + str(row['provider'])\n    \n    # Apply hash function to generate unique ID\n    unique_id = hashlib.sha256(string_to_hash.encode()).hexdigest()\n    \n    return unique_id",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "When data is not uniform across providers, it means that different providers have different data formats, structures, and quality levels for the same type of data. This can make it difficult to compare and analyze data from different providers because the data may not be directly comparable or may require significant preprocessing to make it comparable.\n\nFor example, if you are trying to compare sales data from two different retailers, one retailer may provide sales data in a format that includes more detailed information than the other retailer’s sales data. This can make it difficult to compare the two datasets directly without first preprocessing the data to ensure that they are comparable.\n\nTo circumvent this, there are multiple things to be done. The simplest is to reduce the two datasets to the minimum dimensions and facts required. In this case: transaction_id (the id_hashed column created above), timestamp, customer_id, amount, currency, \n\nIf the two providers have different currency formats, that needs to be resolved. All currencies need to be converted to one currency (e.g. EURO) to allow aggregation / summation by the data analysts. To convert currencies, there are different methods and a decision on which method to choose needs to be a common decision by multiple stakeholders (data team, data / business analysts). One example would be to set up a dataset that ingests daily currency rates using a central bank API. Another would be to use that same dataset to use an average monthly conversion rate.\n\nFor the purpose of this challenge, I'll assume that the data from provider A contains a currency column and an amount column (i.e. the amount is given in local currency). The transformation would be to convert this into a uniform currency (EURO) based on a conversion rate given in the table (average conversion rate corresponding to the month the transaction took place in). The other dataset from Provider B already contains the amounts in EUROs. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def create_amount_eur_column(df):\n    df['amount_eur'] = df['amount'] * df['avg_conversion_rate']\n    return df\n\npayment_provider_a_transformed = create_amount_eur_column(payment_provider_a_transformed)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Another potential discrepancy between the two datasets would be the timestamp / date format of the transaction. It could be that Provider A provides timestamp information about the transaction while Provider B only provides the date the transaction took place. Since the purpose of analysis is to study performance over time, it would suffice to use a \"day\" granularity. This granularity would be the \"least common denominator\" between the two datasets. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def timestamp_to_date(df, column_name):\n    df[\"transaction_date\"] = pd.to_datetime(df[column_name]).dt.date\n    return df\n\npayment_provider_a_transformed = timestamp_to_date(payment_provider_a_transformed, \"timestamp\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "When ```customer_id``` is encrypted in a pandas dataframe, it means that the actual customer ID values are replaced with a different set of values that are not easily recognizable as customer IDs. This is done to protect sensitive information and prevent unauthorized access to customer data.\n\nEncryption is a process of converting plain text into a coded form that cannot be read by unauthorized users.\n\nSince ```customer_id``` is encrypted in both dataframes, it needs to be decrypted. This is to allow joining with the customer table in the internal Postgres database later on. For this example, assume that the Crypto.Cipher module is used to decrypt the values in the specified column using the AES algorithm. The key variable contains the encryption key that was used to encrypt the data. The cipher variable is created using this key and the AES encryption mode. We also assume that both dataframes have been encrypted using the same encryption algorithm. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from Crypto.Cipher import AES\n\ndef decrypt_column(df, column_name):\n    key = b'Sixteen byte key'\n    cipher = AES.new(key, AES.MODE_EAX, nonce=df['nonce'].iloc[0])\n    df[column_name] = df[column_name].apply(lambda x: cipher.decrypt(x).decode('utf-8'))\n    return df\n\npayment_provider_a_transformed = decrypt_column(payment_provider_a_transformed, \"customer_id\")\npayment_provider_b_transformed = decrypt_column(payment_provider_b_transformed, \"customer_id\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Now the two transformed dataframes can be concatenated to build the bigger picture\ndef rename_column(df, old_name, new_name):\n    df.rename(columns={old_name: new_name}, inplace=True)\n    return df\n\ndef select_and_concatenate(df1, df2):\n    selected_cols = ['id_hash', 'customer_id', 'transaction_date', 'amount_eur', 'provider']\n    df1 = df1[selected_cols]\n    df2 = df2[selected_cols]\n    result = pd.concat([df1, df2])\n    return rename_column(result, \"id_hash\", \"transaction_id\")\n\npayment_data_transformed = select_and_concatenate(payment_provider_a_transformed, payment_provider_b_transformed)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Load",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Use the ```psycopg2``` module to connect to a Postgres database and the SQLAlchemy module to create an engine for SQL Alchemy.\n\nThe ```load_dataframe_to_postgres``` function takes two arguments: the pandas dataframe to load into the database and the name of the table to write to.\n\nThe function first connects to the Postgres database using the ```psycopg2.connect``` method. It then creates an engine for SQL Alchemy using the ```create_engine``` method.\n\nThe function checks if the specified table already exists in the database using the engine.has_table method. If it does exist, the function appends the dataframe to the existing table using the df.to_sql method with ```if_exists='append'```. If it doesn’t exist, the function writes the dataframe to a new table using the ```df.to_sql``` method with ```if_exists='replace'```.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import psycopg2\nfrom sqlalchemy import create_engine\n\ndef load_dataframe_to_postgres(df, table_name):\n    # Connect to Postgres database\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"mydatabase\",\n        user=\"myusername\",\n        password=\"mypassword\"\n    )\n\n    # Create engine for SQL Alchemy\n    engine = create_engine('postgresql://myusername:mypassword@localhost/mydatabase')\n\n    # Check if table exists\n    if engine.has_table(table_name):\n        # Append dataframe to existing table\n        df.to_sql(table_name, engine, if_exists='append', index=False)\n    else:\n        # Write dataframe to new table\n        df.to_sql(table_name, engine, if_exists='replace', index=False)\n\n    # Close connection\n    conn.close()\n    \n# Load fact dataframe into Postgres database\nload_dataframe_to_postgres(payment_data_transformed, \"transaction\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "After the fact table - transaction - is written to the database, a BI tool can be used to create the data model. The data model is what the data analysts would be able to access through the BI tool. \n\nThis is a sample SQL query for illustration purpose: \n\n```SQL SELECT *\nFROM transaction t\nLEFT JOIN customer c\nON t.customer_id = c.customer_id;```\n\nThese are sample SQL queries for analysis:\n\n**Question**: \"What is the total amount of transactions month-to-month this year?\"\n\n```SQL \nSELECT DATE_TRUNC('month', transaction_date) AS month, \nSUM(amount) AS total_amount\nFROM transaction\nWHERE EXTRACT(YEAR FROM transaction_date) = EXTRACT(YEAR FROM CURRENT_DATE)\nGROUP BY month```\n\n**Question**: \"What is the average number of transactions performed by customers?\"\n\n```SQL SELECT AVG(num_transactions) AS avg_transactions\nFROM (\n  SELECT COUNT(*) AS num_transactions\n  FROM transaction\n  GROUP BY customer_id\n) subquery```",
      "metadata": {}
    }
  ]
}